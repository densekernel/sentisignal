{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pylab as P \n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from datetime import datetime\n",
    "from yahoo_finance import Share\n",
    "from pandas_datareader import data, wb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to subsample the large data sets for:\n",
    "# specific date range: start_data - end_date ('YYYY-MM-DD')\n",
    "# filters : 'SYMBOL' - list of symbols ['SYM1', SYM2']\n",
    "#'SECTOR' - list of symbols ['Sec1', 'Sec2']\n",
    "#'EXCHANGE' - list of symbols ['Exch1', 'Exch2']\n",
    "def subsample_data(filename_data, filename_symbology, dir_pickle, start_date, end_date, query_attribute, query_criteria, include_avg):\n",
    "    query_criteria_filename = '-'.join(query_criteria[:3])\n",
    "    pickle_name = dir_pickle+'pickle_'+start_date+'_'+end_date+'_'+query_attribute+'_'+query_criteria_filename+'.p'\n",
    "    try: \n",
    "        data = pd.read_pickle(pickle_name)\n",
    "#         print(\"loaded from pickle\")\n",
    "    except:\n",
    "#         print(\"subsampling data from csv\")\n",
    "        # try to read first from pickle\n",
    "        # read csv\n",
    "        data = pd.read_csv(filename_data)\n",
    "        # convert timestamps to datetime objects\n",
    "        data['DATE'] = data['TIMESTAMP_UTC'].apply(lambda x: datetime.strptime(x,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "        data['DATE'] = data['DATE'].apply(lambda x: x.strftime('%x'))\n",
    "        data['DATE'] = data['DATE'].apply(lambda x: pd.to_datetime(x))\n",
    "        # query between start and end date\n",
    "        data = data[(data['DATE'] > start_date) & (data['DATE'] < end_date)]\n",
    "        # merge with symbology csv for additional info\n",
    "        data_symbology = pd.read_csv(filename_symbology)\n",
    "        # convert headers to uppercase for ease of use\n",
    "        data_symbology.columns = [x.upper() for x in data_symbology.columns]\n",
    "        data = pd.merge(data, data_symbology, left_on='SYMBOL', right_on='SYMBOL', how = \"left\")\n",
    "        # remove avg\n",
    "        if include_avg == False:\n",
    "            avg_cols = [col for col in data.columns if 'AVG' in col]\n",
    "            data.drop(avg_cols,inplace=True,axis=1)\n",
    "        # perform filter query based on parameters\n",
    "        data = data[data[query_attribute].isin(query_criteria)]\n",
    "        # save as pickle\n",
    "        data.to_pickle(pickle_name)\n",
    "    # return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data_finance(df):\n",
    "    # log return\n",
    "    df['CHANGE'] = df['ADJ CLOSE'].pct_change()\n",
    "    df['LOG_RETURN'] = np.log(1 + df['CHANGE'])\n",
    "    # volatitility\n",
    "    df['VOLATILITY'] = df['HIGH'] - df['LOW']\n",
    "#     print df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to return historical finance data\n",
    "# takes a list of ticker symbols\n",
    "    # if sum_data_frame is true -> create a summed value to form an indic\n",
    "    # else create a concatenated dataframe\n",
    "def get_data_finance(source, symbols, start_date, end_date, sum_data_frame):\n",
    "    # get finance data using pandas data reader\n",
    "    # create df from first symbol\n",
    "    try:\n",
    "        data_finance = data.DataReader(symbols[0], source, start_date, end_date)\n",
    "        # convert headers to uppercase for ease of use\n",
    "        data_finance.columns = [x.upper() for x in data_finance.columns]\n",
    "        data_finance['SYMBOL'] = symbols[0]\n",
    "        preprocess_data_finance(data_finance)\n",
    "#         print(data_finance.head())\n",
    "    except: \n",
    "        print(\"Unable to retrieve data for: \" + symbols[0])\n",
    "    # reset index (optional)\n",
    "    # data_finance.reset_index(level=0, inplace=True)\n",
    "    # loop through remaining symbols and either add or concatenate\n",
    "    for symbol in symbols[1:]:\n",
    "        print(symbol)\n",
    "        try:\n",
    "            symbol_finance = data.DataReader(symbol, source, start_date, end_date)\n",
    "            # convert headers to uppercase for ease of use\n",
    "            symbol_finance.columns = [x.upper() for x in symbol_finance.columns]\n",
    "            preprocess_data_finance(symbol_finance)\n",
    "            # reset index (optional)\n",
    "            # symbol_finance.reset_index(level=0, inplace=True)\n",
    "            # sum dataframes\n",
    "            if sum_data_frame:\n",
    "                data_finance = data_finance + symbol_finance\n",
    "                print(\"if\")\n",
    "            # vertically concat dataframes\n",
    "            else:\n",
    "                symbol_finance['SYMBOL'] = symbol\n",
    "                data_finance = pd.concat([data_finance, symbol_finance], axis=0)\n",
    "        except: \n",
    "            print(\"Unable to retrieve data for: \" + symbol)\n",
    "    # reset index (optional)\n",
    "    data_finance.reset_index(level=0, inplace=True)\n",
    "    # convert headers to uppercase for ease of use\n",
    "    data_finance.columns = [x.upper() for x in data_finance.columns]\n",
    "    # Set Date as index\n",
    "    # data_finance.index = data_finance['DATE']\n",
    "    # Sort by date\n",
    "    data_finance = data_finance.sort_values(['DATE'], ascending=[True])\n",
    "    # return as dataframe\n",
    "    return data_finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data_sentiment(df):\n",
    "    # log bullishness\n",
    "    df['LOG_BULLISHNESS'] = np.log(1 + df['BULL_SCORED_MESSAGES']) - np.log(1 + df['BEAR_SCORED_MESSAGES'])\n",
    "    # log bull bear ratio\n",
    "    df['LOG_BULL_BEAR_RATIO'] = np.log(df['BULL_SCORED_MESSAGES']) / np.log(df['BEAR_SCORED_MESSAGES'])\n",
    "    # TISf\n",
    "    df['TISf'] = (1+df['BULL_SCORED_MESSAGES'])/(1+ df['BULL_SCORED_MESSAGES']+df['BEAR_SCORED_MESSAGES'])\n",
    "    # RTISf\n",
    "    df['RTISf'] = ((1+df['BULL_SCORED_MESSAGES'])/(1+ df['BULL_SCORED_MESSAGES']+df['BEAR_SCORED_MESSAGES'])).pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_sentiment_finance(data_sentiment, data_finance, with_symbol, fill_finance, fill_sentiment):\n",
    "    if with_symbol:\n",
    "#         return pd.merge(data_sentiment, data_finance, on=['DATE', 'SYMBOL'], how='left')\n",
    "        if fill_finance:\n",
    "            return pd.merge(data_sentiment, data_finance, on=['DATE', 'SYMBOL'], how='left')\n",
    "        if fill_sentiment:\n",
    "            return pd.merge(data_sentiment, data_finance, on=['DATE', 'SYMBOL'], how='left')\n",
    "    else:\n",
    "        if fill_finance:\n",
    "            return pd.merge(data_sentiment, data_finance, on=['DATE'], how='left')\n",
    "        if fill_sentiment:\n",
    "            return pd.merge(data_sentiment, data_finance, on=['DATE'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlation_analysis(df):\n",
    "    i = 0\n",
    "    resCorr = pd.DataFrame(index = ['BULLISH_INTENSITY','BEARISH_INTENSITY','BULL_MINUS_BEAR','BULL_SCORED_MESSAGES',\n",
    "       'BEAR_SCORED_MESSAGES','LOG_BULLISHNESS','TISf','RTISf'] ,columns = ['LOG_RETURN','VOLUME'])\n",
    "    for column in resCorr.index:\n",
    "        resCorr.ix[i,0] = df['LOG_RETURN'].corr(df[column])\n",
    "        resCorr.ix[i,1] = df['VOLUME'].corr(df[column])\n",
    "        i += 1\n",
    "    return resCorr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
